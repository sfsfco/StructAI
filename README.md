# StructAI â€“ Scalable AI Platform for Structured Data Extraction
### Project Under Construction.
**StructAI** is a production-inspired backend platform that transforms unstructured text (reports, notes, contracts, emails) into structured, machine-readable data.
It combines **OpenAI-powered LLMs** with **Google LangExtract** for reliable information extraction, and is built with modern, scalable backend architecture: **FastAPI**, **FAISS**, **Redis**, **PostgreSQL**, background workers, and **Docker Compose**.

> *â€œCooking up a scalable AI system â€” fresh out of the oven.â€* ğŸ°

---

## âœ¨ Features

* ğŸ§  **AI-Powered Extraction** â€“ Structured data extraction using OpenAI LLMs + Google LangExtract
* ğŸ” **Semantic Retrieval** â€“ FAISS vector search for efficient document retrieval
* âš¡ **Async Processing** â€“ Background workers for heavy ingestion & embedding tasks
* ğŸ§° **Caching** â€“ Redis cache to reduce latency and LLM costs
* ğŸ—„ï¸ **Persistence** â€“ PostgreSQL for metadata and job tracking
* ğŸ³ **Dockerized** â€“ One-command local setup with Docker Compose
* ğŸ“ˆ **Scalable by Design** â€“ Stateless API, isolated services, horizontal scaling ready
* ğŸ§ª **Testable Architecture** â€“ Unit, integration, and E2E testing strategy

---

## ğŸ—ï¸ Architecture (High-Level)

```text
[ Client ]
    |
    v
[ FastAPI API Gateway ]
    |
    +--> [ LangExtract Service ] ---> [ OpenAI API (External) ]
    |
    +--> [ Embedding Service ] ---> [ FAISS Vector Store ]
    |
    +--> [ Redis Cache & Queue Broker ]
    |
    +--> [ PostgreSQL ]
    |
    +--> [ Background Workers ]
```

**Key Design Principles:**

* Stateless API â†’ horizontal scaling
* AI provider abstracted behind a service layer
* Async workers â†’ API remains responsive under load
* Vector search â†’ scalable retrieval over large corpora

---

## ğŸ§° Tech Stack

* **Backend:** Python 3.11, FastAPI
* **LLM:** OpenAI API (external)
* **Extraction:** Google LangExtract
* **Vector Store:** FAISS
* **Cache & Queue Broker:** Redis
* **Database:** PostgreSQL
* **Workers:** Celery or RQ
* **Containerization:** Docker, Docker Compose
* **Testing:** pytest

---

## ğŸ“¦ Project Structure

```text
ai-extraction-platform/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/                    # Routes
â”‚   â”œâ”€â”€ services/               # LLM, LangExtract, FAISS, Redis
â”‚   â”œâ”€â”€ workers/                # Background tasks
â”‚   â”œâ”€â”€ db/                     # DB session & models
â”‚   â””â”€â”€ schemas/                # Pydantic schemas
â”œâ”€â”€ Dockerfile                  # API container
â”œâ”€â”€ Dockerfile.worker           # Worker container
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started (Docker)

### 1ï¸âƒ£ Prerequisites

* Docker & Docker Compose
* OpenAI API Key

### 2ï¸âƒ£ Setup Environment

Create a `.env` file:

```env
OPENAI_API_KEY=your_api_key_here

POSTGRES_USER=ai_user
POSTGRES_PASSWORD=ai_pass
POSTGRES_DB=ai_db

REDIS_HOST=redis
REDIS_PORT=6379
```

> âš ï¸ Never commit your real API key. `.env` is ignored by git.

### 3ï¸âƒ£ Run the Stack

```bash
docker-compose up --build
```

API will be available at:

```
http://localhost:8000
```

Health check:

```
GET /health
```

---

## ğŸ”Œ API Examples

### Index a Document (Async)

```http
POST /documents/index
{
  "document_id": "doc_123",
  "text": "Long unstructured document..."
}
```

### Extract Structured Data

```http
POST /extract
{
  "document_id": "doc_123",
  "instructions": "Extract names, dates, and risks"
}
```

**Response:**

```json
{
  "entities": [
    {
      "name": "John Doe",
      "date": "2024-01-12",
      "risk": "Payment delay"
    }
  ]
}
```

---

## ğŸ§ª Testing Strategy

### Unit Tests

* LLM client (mock OpenAI responses)
* LangExtract service
* FAISS vector store wrapper
* Redis cache layer

### Integration Tests

* FastAPI endpoints
* PostgreSQL + Redis integration
* FAISS indexing & retrieval

### End-to-End (E2E)

* Ingest document â†’ embed â†’ index â†’ extract â†’ validate response

```bash
pytest
```

---

## ğŸ“ˆ Scalability & System Design

* **Stateless API** â†’ scale horizontally behind a load balancer
* **Worker Pool** â†’ scale ingestion and embedding independently
* **Redis Cache** â†’ reduce repeated LLM calls and latency
* **FAISS Abstraction** â†’ can be replaced by a managed vector DB in production
* **Rate Limiting & Backpressure** â†’ protect LLM costs and system stability
* **Future Kubernetes Deployment** â†’ production-ready migration path

> *â€œIngestion, retrieval, and extraction pipelines are decoupled to enable independent scaling under high load.â€*

---

## ğŸ” Security Notes

* Secrets managed via environment variables
* No API keys committed to the repository
* Rate limiting on API endpoints
* Input size limits to prevent abuse
* Logs exclude sensitive content

---

## ğŸ›£ï¸ Roadmap / Future Improvements

* Replace FAISS with managed vector DB (e.g., Pinecone, Weaviate)
* Add Kubernetes manifests
* Observability: metrics, tracing, dashboards
* Multi-LLM provider support
* Streaming ingestion & real-time extraction

---

## ğŸ§‘â€ğŸ³ Why This Project?

This project goes beyond simple â€œLLM callsâ€ and demonstrates:

* Real-world **AI system design**
* **Production-style backend architecture**
* **Scalability, async processing, caching, and vector search**
* Practical integration of **Google LangExtract** with LLMs

