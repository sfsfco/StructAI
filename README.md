# StructAI â€“ Scalable AI Platform for Structured Data Extraction

**StructAI** is a production-grade backend platform that transforms unstructured text (reports, contracts, emails, notes) into structured, machine-readable data. It combines **OpenAI-powered LLMs** with a **RAG pipeline** (Retrieval-Augmented Generation) for reliable information extraction, built on modern, scalable backend architecture.

> Ingestion â†’ Chunking â†’ Embedding â†’ FAISS Retrieval â†’ LLM Extraction â†’ Structured JSON

---

## Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Quick Start (Docker)](#-quick-start-docker)
- [Configuration Reference](#ï¸-configuration-reference)
- [API Reference](#-api-reference)
- [Processing Pipeline](#-processing-pipeline)
- [Observability](#-observability)
- [Testing](#-testing)
- [Scaling & Production](#-scaling--production)
- [Kubernetes Deployment](#ï¸-kubernetes-deployment)
- [Security](#-security)
- [Roadmap](#ï¸-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## âœ¨ Features

- ðŸ§  **AI-Powered Extraction** â€“ Structured data extraction via OpenAI LLMs with schema-guided prompts
- ðŸ”Ž **Semantic Retrieval** â€“ FAISS vector search finds the most relevant text chunks for each query
- âš¡ **Async Pipeline** â€“ Celery background workers handle chunking, embedding, and indexing
- ðŸ§° **Multi-Tier Caching** â€“ Redis caches extractions, embeddings, and deduplication lookups
- ðŸ›¡ï¸ **Backpressure Protection** â€“ In-flight and queue-depth limits prevent overload
- ðŸ“Š **Observability** â€“ Prometheus metrics, Grafana dashboards, structured JSON logging, correlation IDs
- ðŸ“ˆ **Horizontally Scalable** â€“ Stateless API + independently scalable workers
- ðŸ”Œ **Pluggable Vector Store** â€“ Abstract interface allows swapping FAISS for Pinecone/Qdrant/Weaviate
- ðŸ³ **Production Docker Setup** â€“ Nginx load balancer, Gunicorn multi-worker, resource limits
- â˜¸ï¸ **Kubernetes Ready** â€“ Full K8s manifests with HPA, probes, and PVCs

---

## ðŸ—ï¸ Architecture

```text
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Client    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Nginx     â”‚  (LB + rate limit)
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚              â”‚              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
          â”‚  API Pod 1  â”‚â”‚  API Pod 2 â”‚â”‚  API Pod N  â”‚
          â”‚  (gunicorn) â”‚â”‚  (gunicorn)â”‚â”‚  (gunicorn) â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                      â”‚                      â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Redis     â”‚       â”‚ PostgreSQL  â”‚       â”‚ Celery Workers â”‚
  â”‚ cache+brokerâ”‚       â”‚  metadata   â”‚       â”‚ indexing: 2-8  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ default:  1-4  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  FAISS Index   â”‚
                                              â”‚ (shared volume)â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  OpenAI API    â”‚
                                              â”‚  (external)    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Design Principles:**

| Principle | How |
|-----------|-----|
| Stateless API | No in-process state â†’ scale horizontally |
| Service abstraction | LLM + vector store behind interfaces â†’ swap providers |
| Async processing | Heavy work offloaded to Celery â†’ API stays responsive |
| Defence-in-depth | Rate limiting at Nginx + app + backpressure middleware |
| Repository pattern | DB access abstracted â†’ testable + clean domain logic |
| Dependency injection | FastAPI `Depends()` â†’ services swappable in tests |

---

## ðŸ§° Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| API Framework | FastAPI 0.115 | Async REST API with OpenAPI docs |
| ASGI Server | Gunicorn + Uvicorn | Multi-worker production server |
| LLM Provider | OpenAI API | Chat completions + embeddings |
| Vector Store | FAISS (faiss-cpu) | In-process cosine similarity search |
| Cache & Broker | Redis 7 | Result cache + Celery message broker |
| Database | PostgreSQL 15 | Document metadata, chunks, extractions |
| Background Jobs | Celery 5.4 | Task queue with retries + dead-letter |
| Monitoring | Prometheus + Grafana | Metrics collection + dashboards |
| Task Dashboard | Flower | Celery monitoring UI |
| Load Balancer | Nginx | Reverse proxy, rate limiting, compression |
| Logging | structlog | Structured JSON logs with correlation IDs |
| Config | pydantic-settings | Type-safe env-based configuration |
| Containerisation | Docker + Compose | Local dev + production deployment |
| Orchestration | Kubernetes (optional) | HPA, rolling updates, probes |
| Testing | pytest + httpx | Unit, integration, E2E with async support |

---

## ðŸ“¦ Project Structure

```text
StructAI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI application entrypoint
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py              # Environment configuration (pydantic-settings)
â”‚   â”‚   â”œâ”€â”€ gunicorn_conf.py       # Gunicorn production config
â”‚   â”‚   â”œâ”€â”€ logging.py             # Structured logging + correlation IDs
â”‚   â”‚   â””â”€â”€ metrics.py             # Prometheus metrics registry
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py              # REST endpoints
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ backpressure.py        # In-flight + queue-depth load shedding
â”‚   â”‚   â”œâ”€â”€ correlation.py         # X-Correlation-ID propagation
â”‚   â”‚   â”œâ”€â”€ metrics.py             # Per-request Prometheus metrics
â”‚   â”‚   â””â”€â”€ rate_limit.py          # SlowAPI per-IP rate limiting
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_client.py          # OpenAI client abstraction (BaseLLMClient)
â”‚   â”‚   â”œâ”€â”€ langextract_service.py # Structured extraction via LLM
â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # Text â†’ vector embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # BaseVectorStore + FAISS implementation
â”‚   â”‚   â”œâ”€â”€ cache_service.py       # Multi-tier Redis cache
â”‚   â”‚   â””â”€â”€ dependencies.py        # FastAPI dependency injection
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â””â”€â”€ tasks.py               # Celery task definitions + pipeline
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ models.py              # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ repository.py          # Repository layer (data access)
â”‚   â”‚   â””â”€â”€ session.py             # Async engine + session factory
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ extract.py             # Pydantic request/response schemas
â”œâ”€â”€ worker/
â”‚   â””â”€â”€ worker.py                  # Celery worker entrypoint
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                # Shared fixtures
â”‚   â”œâ”€â”€ unit/                      # Unit tests (mocked dependencies)
â”‚   â”œâ”€â”€ integration/               # Integration tests (real DB/Redis)
â”‚   â””â”€â”€ e2e/                       # End-to-end pipeline tests
â”œâ”€â”€ k8s/                           # Kubernetes manifests
â”‚   â”œâ”€â”€ namespace.yml
â”‚   â”œâ”€â”€ configmap.yml
â”‚   â”œâ”€â”€ secret.yml
â”‚   â”œâ”€â”€ api-deployment.yml         # API Deployment + PVC for FAISS
â”‚   â”œâ”€â”€ api-service.yml            # ClusterIP service
â”‚   â”œâ”€â”€ api-hpa.yml                # HPA: 2â†’10 pods on CPU/memory
â”‚   â”œâ”€â”€ worker-deployment.yml      # Indexing + default workers + Beat
â”‚   â”œâ”€â”€ worker-hpa.yml             # Worker HPA: 1â†’8 pods on CPU
â”‚   â”œâ”€â”€ redis-deployment.yml       # Redis + PVC + service
â”‚   â””â”€â”€ postgres-deployment.yml    # PostgreSQL + PVC + service
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf                 # Reverse proxy + load balancer
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml             # Scrape configuration
â”‚   â””â”€â”€ grafana/provisioning/      # Grafana datasource provisioning
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_tests.sh               # Test runner helper
â”œâ”€â”€ Dockerfile                     # API image (multi-stage, gunicorn)
â”œâ”€â”€ Dockerfile.worker              # Worker image (multi-stage, celery)
â”œâ”€â”€ docker-compose.yml             # Full dev stack
â”œâ”€â”€ docker-compose.override.yml    # Dev overrides (hot reload, bind mount)
â”œâ”€â”€ docker-compose.prod.yml        # Production (Nginx, split workers)
â”œâ”€â”€ docker-compose.test.yml        # Test DB + Redis on separate ports
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ pytest.ini                     # pytest configuration
â”œâ”€â”€ .env.example                   # Environment variable template
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start (Docker)

### Prerequisites

- Docker & Docker Compose v2+
- An OpenAI API key ([get one here](https://platform.openai.com/api-keys))

### 1. Clone & Configure

```bash
git clone https://github.com/your-org/StructAI.git
cd StructAI

# Create your environment file from the template
cp .env.example .env

# Edit .env and set your OpenAI API key
```

### 2. Start the Stack (Development)

```bash
# Build and start all services
docker compose up --build

# Or run detached
docker compose up -d --build
```

Services available at:

| Service | URL | Description |
|---------|-----|-------------|
| **API** | http://localhost:8000 | FastAPI application |
| **API Docs** | http://localhost:8000/docs | Interactive Swagger UI |
| **ReDoc** | http://localhost:8000/redoc | Alternative API docs |
| **Flower** | http://localhost:5555 | Celery task monitoring |
| **Prometheus** | http://localhost:9090 | Metrics query dashboard |
| **Grafana** | http://localhost:3000 | Visualisation (admin/admin) |
| PostgreSQL | localhost:5432 | Database |
| Redis | localhost:6379 | Cache + broker |

### 3. Start the Stack (Production)

```bash
# Production: Nginx load balancer + Gunicorn + split workers
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build

# Scale API replicas behind Nginx
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale api=3

# Scale indexing workers independently
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale worker-indexing=4
```

### 4. Verify

```bash
# Liveness check
curl http://localhost:8000/api/v1/health | python3 -m json.tool

# Readiness check (verifies all dependencies)
curl http://localhost:8000/api/v1/ready | python3 -m json.tool
```

### 5. Tear Down

```bash
docker compose down        # Stop services
docker compose down -v     # Stop + remove volumes (clean slate)
```

---

## âš™ï¸ Configuration Reference

All configuration is managed via environment variables (loaded from `.env` via `pydantic-settings`):

### Application

| Variable | Default | Description |
|----------|---------|-------------|
| `APP_NAME` | `StructAI` | Application name |
| `APP_VERSION` | `0.1.0` | Application version |
| `DEBUG` | `false` | Enable debug mode (coloured logs, SQL echo) |
| `LOG_LEVEL` | `INFO` | Logging level |

### OpenAI

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | *(required)* | OpenAI API key |
| `OPENAI_MODEL` | `gpt-4o` | Chat completion model |
| `OPENAI_EMBEDDING_MODEL` | `text-embedding-3-small` | Embedding model |
| `OPENAI_MAX_TOKENS` | `4096` | Max tokens per completion |
| `OPENAI_TEMPERATURE` | `0.0` | LLM temperature (0 = deterministic) |

### Database

| Variable | Default | Description |
|----------|---------|-------------|
| `POSTGRES_USER` | `ai_user` | Database user |
| `POSTGRES_PASSWORD` | `ai_pass` | Database password |
| `POSTGRES_HOST` | `db` | Database host |
| `POSTGRES_PORT` | `5432` | Database port |
| `POSTGRES_DB` | `ai_db` | Database name |

### Redis

| Variable | Default | Description |
|----------|---------|-------------|
| `REDIS_HOST` | `redis` | Redis host |
| `REDIS_PORT` | `6379` | Redis port |
| `REDIS_DB` | `0` | Redis database index |
| `REDIS_CACHE_TTL` | `3600` | Default cache TTL (seconds) |
| `EMBEDDING_CACHE_TTL` | `86400` | Embedding cache TTL (24 hours) |

### Vector Store

| Variable | Default | Description |
|----------|---------|-------------|
| `FAISS_INDEX_DIR` | `/app/data/faiss` | FAISS index storage path |
| `FAISS_DIMENSION` | `1536` | Embedding vector dimension |
| `VECTOR_STORE_BACKEND` | `faiss` | Vector store backend (`faiss`, `pinecone`, etc.) |

### Processing & Limits

| Variable | Default | Description |
|----------|---------|-------------|
| `CHUNK_SIZE` | `512` | Document chunk size (characters) |
| `CHUNK_OVERLAP` | `64` | Chunk overlap (characters) |
| `RATE_LIMIT_DEFAULT` | `60/minute` | Global rate limit per IP |
| `RATE_LIMIT_EXTRACT` | `10/minute` | Extraction endpoint rate limit |
| `BACKPRESSURE_MAX_INFLIGHT` | `100` | Max concurrent API requests |
| `BACKPRESSURE_MAX_QUEUE_DEPTH` | `500` | Max pending queue tasks |

### Production Server

| Variable | Default | Description |
|----------|---------|-------------|
| `WEB_CONCURRENCY` | `CPUÃ—2+1` | Gunicorn worker count |
| `MAX_REQUESTS` | `1000` | Worker recycling threshold |
| `MAX_REQUESTS_JITTER` | `50` | Recycling jitter |
| `WORKER_TIMEOUT` | `120` | Request timeout (seconds) |

---

## ðŸ”Œ API Reference

**Base path:** `/api/v1`  
**Interactive docs:** http://localhost:8000/docs

### Health & Readiness

#### `GET /api/v1/health` â€” Liveness Probe

Lightweight check for Kubernetes liveness. Returns status, version, and dependency states.

```bash
curl http://localhost:8000/api/v1/health
```

```json
{
  "status": "ok",
  "version": "0.1.0",
  "uptime_seconds": 3642.15,
  "db": "connected",
  "redis": "connected",
  "faiss_index_loaded": true
}
```

#### `GET /api/v1/ready` â€” Readiness Probe

Deep check â€” verifies PostgreSQL, Redis, Celery broker, and FAISS. Returns **HTTP 503** if not ready.

```bash
curl http://localhost:8000/api/v1/ready
```

```json
{
  "ready": true,
  "checks": {
    "postgres": { "status": "ok" },
    "redis": { "status": "ok" },
    "celery_broker": { "status": "ok" },
    "faiss": { "status": "ok" }
  },
  "version": "0.1.0"
}
```

---

### Document Indexing

#### `POST /api/v1/documents/index` â€” Ingest a Document

Accepts raw text, persists metadata to PostgreSQL, and enqueues background processing.

**Request:**

```bash
curl -X POST http://localhost:8000/api/v1/documents/index \
  -H "Content-Type: application/json" \
  -d '{
    "filename": "contract.pdf",
    "content": "This Software License Agreement is entered into as of January 15, 2025, between Acme Corp (\"Licensor\") and Beta Inc (\"Licensee\"). The Licensee agrees to pay $50,000 annually. The agreement is effective for 3 years with automatic renewal unless terminated with 90 days notice."
  }'
```

**Response (HTTP 202 Accepted):**

```json
{
  "document_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "status": "pending",
  "message": "Document queued for indexing"
}
```

| Error | Reason |
|-------|--------|
| 409 | Duplicate document (identical content hash already indexed) |
| 429 | Rate limit exceeded |
| 503 | Processing queue full (backpressure) |

---

### Task Status

#### `GET /api/v1/tasks/{task_id}` â€” Poll Background Task

Check the status of a document indexing pipeline.

```bash
curl http://localhost:8000/api/v1/tasks/{task_id}
```

```json
{
  "task_id": "abc123-def456",
  "status": "SUCCESS",
  "result": {
    "document_id": "a1b2c3d4-...",
    "status": "indexed",
    "chunks": 12
  },
  "date_done": "2025-06-15T10:30:00Z"
}
```

**Task states:** `PENDING` â†’ `STARTED` â†’ `SUCCESS` | `FAILURE` | `RETRY`

---

### Structured Extraction

#### `POST /api/v1/extract` â€” Extract Data from a Document

Runs the RAG pipeline: embed query â†’ FAISS search â†’ retrieve chunks â†’ LLM extraction â†’ cache result.

**Request:**

```bash
curl -X POST http://localhost:8000/api/v1/extract \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "query": "Extract all parties, effective date, payment terms, and renewal policy",
    "schema_hint": {
      "parties": ["string"],
      "effective_date": "string",
      "annual_payment": "string",
      "duration": "string",
      "renewal_policy": "string"
    }
  }'
```

**Response:**

```json
{
  "extraction_id": "f7e8d9c0-b1a2-3456-cdef-7890abcdef12",
  "document_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "query": "Extract all parties, effective date, payment terms, and renewal policy",
  "result": {
    "parties": ["Acme Corp (Licensor)", "Beta Inc (Licensee)"],
    "effective_date": "January 15, 2025",
    "annual_payment": "$50,000",
    "duration": "3 years",
    "renewal_policy": "Automatic renewal unless terminated with 90 days notice"
  },
  "model_used": "gpt-4o",
  "latency_ms": 2450.32,
  "cached": false
}
```

Subsequent identical queries return `"cached": true` with sub-millisecond latency.

The optional `schema_hint` field guides the LLM to produce output matching the specified shape.

| Error | Reason |
|-------|--------|
| 400 | Document not yet indexed (status â‰  `indexed`) |
| 404 | Document not found or no relevant chunks found |
| 429 | Rate limit exceeded |

---

### Metrics

#### `GET /metrics` â€” Prometheus Scrape Endpoint

Returns all application metrics in Prometheus text exposition format.

```bash
curl http://localhost:8000/metrics
```

---

## ðŸ”„ Processing Pipeline

When a document is submitted via `POST /documents/index`, the following Celery chain executes:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. chunk_doc    â”‚â”€â”€â”€â–¶â”‚ 2. gen_embeddings  â”‚â”€â”€â”€â–¶â”‚ 3. index_vecs â”‚â”€â”€â”€â–¶â”‚ 4. finalise_doc  â”‚
â”‚                 â”‚    â”‚                    â”‚    â”‚               â”‚    â”‚                  â”‚
â”‚ Split text into â”‚    â”‚ Call OpenAI embed  â”‚    â”‚ Add to FAISS  â”‚    â”‚ Set status =     â”‚
â”‚ overlapping     â”‚    â”‚ API for each chunk â”‚    â”‚ Persist chunk â”‚    â”‚ "indexed" in DB  â”‚
â”‚ chunks          â”‚    â”‚ L2-normalise vecs  â”‚    â”‚ metadata in DBâ”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                        â”‚                      â”‚
    queue: indexing         queue: indexing          queue: indexing        queue: default
```

**Reliability features:**

| Feature | Implementation |
|---------|---------------|
| Independent retries | Each stage retries 3Ã— with exponential backoff (30s â†’ 60s â†’ 120s) |
| Idempotency | Re-running deletes old chunks and recreates â€” safe to retry |
| Error callback | On final failure, document status â†’ `failed` + structured error log |
| Queue routing | Heavy work â†’ `indexing` queue; light work â†’ `default` queue |
| Worker crash safety | `task_acks_late=True` + `task_reject_on_worker_lost=True` |
| Memory leak prevention | `max-tasks-per-child` recycles workers after N tasks |
| Metrics | Per-task duration, success/failure counts, in-progress gauge |

**Periodic maintenance tasks** (via Celery Beat):

| Task | Schedule | Purpose |
|------|----------|---------|
| `cleanup_failed_docs` | Every 24 hours | Remove documents stuck in `failed` status > 7 days |
| `optimise_faiss_index` | Every 1 hour | Re-save FAISS index for disk compaction |

---

## ðŸ“Š Observability

### Structured Logging

All logs are JSON-formatted in production and coloured console in debug mode, using `structlog`:

```json
{
  "event": "llm.chat_completion.done",
  "model": "gpt-4o",
  "usage": {"prompt_tokens": 1250, "completion_tokens": 340},
  "correlation_id": "a3f2b1c0d4e5",
  "timestamp": "2025-06-15T10:30:00.000Z",
  "level": "info"
}
```

Every request receives a **correlation ID** (`X-Correlation-ID` header) that:
- Is propagated through all logs during the request
- Is returned in the response headers
- Can be sent by the client to trace a request through the system

### Prometheus Metrics

Key metrics exposed at `/metrics`:

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `structai_http_request_duration_seconds` | Histogram | method, path, status_code | Request latency |
| `structai_http_requests_total` | Counter | method, path, status_code | Total request count |
| `structai_http_requests_in_progress` | Gauge | method | Current in-flight requests |
| `structai_llm_call_duration_seconds` | Histogram | operation, model | LLM API call latency |
| `structai_llm_calls_total` | Counter | operation, model, status | LLM call count |
| `structai_llm_tokens_total` | Counter | model, type | Token consumption |
| `structai_cache_ops_total` | Counter | operation, result | Cache hit/miss/set counts |
| `structai_faiss_search_duration_seconds` | Histogram | â€” | Vector search latency |
| `structai_faiss_index_size_vectors` | Gauge | â€” | Current index size |
| `structai_tasks_total` | Counter | task_name, status | Background task count |
| `structai_task_duration_seconds` | Histogram | task_name, status | Task execution time |
| `structai_documents_indexed_total` | Counter | â€” | Successfully indexed docs |
| `structai_documents_failed_total` | Counter | â€” | Failed indexing attempts |

### Monitoring Stack

| Service | URL | Purpose |
|---------|-----|---------|
| Prometheus | http://localhost:9090 | Metric collection & querying |
| Grafana | http://localhost:3000 | Dashboards & alerting (admin/admin) |
| Flower | http://localhost:5555 | Celery task monitoring |

Prometheus is pre-configured to scrape the API at 15-second intervals.

---

## ðŸ§ª Testing

### Test Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Test Pyramid                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â–²  E2E Tests          (tests/e2e/)            â”‚
â”‚  â”‚  Full pipeline: index â†’ extract â†’ verify    â”‚
â”‚  â”‚                                             â”‚
â”‚  â”‚  Integration Tests   (tests/integration/)   â”‚
â”‚  â”‚  Real DB + Redis, FastAPI TestClient         â”‚
â”‚  â”‚                                             â”‚
â”‚  â”‚  Unit Tests          (tests/unit/)          â”‚
â”‚  â”‚  Mocked dependencies, fast, isolated         â”‚
â”‚  â–¼                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Running Tests

```bash
# â”€â”€ Unit Tests (no external dependencies) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pytest tests/unit/ -v

# â”€â”€ Integration Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Start isolated test DB + Redis (separate ports to avoid conflicts)
docker compose -f docker-compose.test.yml up -d

# Run integration tests
TEST_DATABASE_URL=postgresql+asyncpg://ai_user:ai_pass@localhost:5433/ai_db_test \
TEST_REDIS_URL=redis://localhost:6380/1 \
pytest tests/integration/ -v

# Tear down test dependencies
docker compose -f docker-compose.test.yml down -v

# â”€â”€ E2E Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pytest tests/e2e/ -v

# â”€â”€ All Tests with Coverage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pytest --cov=app --cov-report=term-missing --cov-report=html

# â”€â”€ Via Helper Script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
./scripts/run_tests.sh
```

### Test Examples

**Unit test** â€” mocked LLM client:

```python
@pytest.mark.asyncio
async def test_extraction_returns_parsed_json(mock_llm_client):
    mock_llm_client.chat_completion.return_value = '{"name": "John", "age": 30}'
    service = LangExtractService(mock_llm_client)

    result = await service.extract(["Some text about John..."], "Extract name and age")

    assert result == {"name": "John", "age": 30}
    mock_llm_client.chat_completion.assert_called_once()
```

**Integration test** â€” real FastAPI + database:

```python
@pytest.mark.asyncio
async def test_index_document_returns_202(async_client, db_session):
    response = await async_client.post("/api/v1/documents/index", json={
        "filename": "test.txt",
        "content": "Hello world, this is a test document."
    })

    assert response.status_code == 202
    data = response.json()
    assert data["status"] == "pending"
    assert "document_id" in data
```

**E2E test** â€” full pipeline:

```python
@pytest.mark.asyncio
async def test_full_extraction_pipeline(async_client):
    # 1. Index a document
    idx = await async_client.post("/api/v1/documents/index", json={
        "filename": "contract.pdf",
        "content": "Agreement between Acme Corp and Beta Inc..."
    })
    doc_id = idx.json()["document_id"]

    # 2. Wait for background processing to complete
    await wait_for_document_status(doc_id, "indexed", timeout=30)

    # 3. Extract structured data
    ext = await async_client.post("/api/v1/extract", json={
        "document_id": doc_id,
        "query": "Extract all parties mentioned"
    })

    assert ext.status_code == 200
    result = ext.json()
    assert result["cached"] is False
    assert "result" in result

    # 4. Verify cache works on repeat query
    ext2 = await async_client.post("/api/v1/extract", json={
        "document_id": doc_id,
        "query": "Extract all parties mentioned"
    })
    assert ext2.json()["cached"] is True
```

### What Each Test Layer Covers

| Layer | Tests | Dependencies |
|-------|-------|-------------|
| **Unit** | LLM client, embedding service, LangExtract service, cache service, vector store, chunking, metrics | None (all mocked) |
| **Integration** | API endpoints, DB repository, Redis cache, FAISS indexing + retrieval | Test PostgreSQL + Redis |
| **E2E** | Full pipeline: index â†’ process â†’ extract â†’ cache hit | All services running |

---

## ðŸ“ˆ Scaling & Production

### Horizontal Scaling of the API

The API is **stateless** â€” scale horizontally with zero configuration changes:

```bash
# Docker Compose: scale behind Nginx load balancer
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale api=3

# Kubernetes: HPA auto-scales 2â†’10 pods based on CPU/memory
kubectl apply -f k8s/api-hpa.yml
```

**Production server** uses Gunicorn with uvicorn workers:
- `preload_app=True` â€” shares FAISS index across workers via copy-on-write
- `MAX_REQUESTS=2000` â€” recycles workers to prevent memory leaks
- `PROMETHEUS_MULTIPROC_DIR` â€” enables metrics in multi-process mode
- Nginx least-connections load balancing across replicas

### Scaling Workers Independently

Workers are split by queue type for independent scaling:

| Queue | Worker | Workload | Scale Strategy |
|-------|--------|----------|----------------|
| `indexing` | worker-indexing | Embedding generation + FAISS writes | CPU/memory-bound â†’ scale on utilisation |
| `default` | worker-default | Finalisation, pipeline orchestration | I/O-bound â†’ fewer instances needed |
| `maintenance` | worker-default | Cleanup, FAISS compaction | Runs on default workers |

```bash
# Scale only the heavy indexing workers
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale worker-indexing=4
```

### Caching: Three Tiers to Reduce LLM Cost

| Tier | What | Key Pattern | TTL | Benefit |
|------|------|-------------|-----|---------|
| 1 | Extraction results | `extract:{doc_id}:{query_hash}` | 1 hour | Identical queries served instantly |
| 2 | Embedding vectors | `emb:{model}:{text_hash}` | 24 hours | Skip redundant OpenAI embedding calls |
| 3 | Content dedup | `dedup:{content_hash}` | 7 days | Skip re-indexing identical documents |

Additional features: bulk invalidation on re-index, cache stats for monitoring, LRU eviction under memory pressure.

### Rate Limiting & Backpressure

| Layer | Mechanism | Limits |
|-------|-----------|--------|
| **Nginx** | `limit_req_zone` | 10 req/s general, 2 req/s extraction |
| **SlowAPI** | Per-IP rate limiting | 60/min global, 10/min extraction |
| **Backpressure** | In-flight + queue depth | 503 when > 100 inflight or > 500 queued |

All layers return `Retry-After` headers so clients can implement exponential backoff.

### Swapping the Vector Store

The vector store uses an abstract interface (`BaseVectorStore`):

```python
class BaseVectorStore(ABC):
    def add(self, vectors, metadata=None) -> List[int]: ...
    def search(self, query_vector, k=5) -> List[Tuple[int, float]]: ...
    def delete(self, ids) -> int: ...
    def save(self) -> None: ...
    def reset(self) -> None: ...
```

To migrate: implement a new subclass, set `VECTOR_STORE_BACKEND=pinecone` (or `qdrant`, `weaviate`), done.

**When to migrate from FAISS:** index exceeds available RAM, need metadata filtering, need multi-node writes, or hosting in ephemeral/serverless environments.

---

## â˜¸ï¸ Kubernetes Deployment

Full manifests in `k8s/`:

```bash
kubectl apply -f k8s/namespace.yml
kubectl apply -f k8s/secret.yml          # Populate with real values first
kubectl apply -f k8s/configmap.yml
kubectl apply -f k8s/postgres-deployment.yml
kubectl apply -f k8s/redis-deployment.yml
kubectl apply -f k8s/api-deployment.yml
kubectl apply -f k8s/api-service.yml
kubectl apply -f k8s/api-hpa.yml
kubectl apply -f k8s/worker-deployment.yml
kubectl apply -f k8s/worker-hpa.yml
```

| Feature | Details |
|---------|---------|
| **HPA** | API: 2â†’10 pods on CPU/memory. Workers: 1â†’8 pods on CPU |
| **Rolling updates** | Zero-downtime with `maxUnavailable: 0`, `maxSurge: 1` |
| **Three probe types** | Startup, liveness (`/health`), readiness (`/ready`) |
| **Resource limits** | CPU/memory requests and limits on every container |
| **PVC** | FAISS index shared via `ReadWriteMany` PersistentVolumeClaim |
| **Secrets** | Sensitive values in K8s Secrets |
| **Beat singleton** | `replicas: 1` + `Recreate` strategy for the scheduler |

For queue-depth-based autoscaling, add [KEDA](https://keda.sh) with a Redis scaler.

---

## ðŸ” Security

| Measure | Implementation |
|---------|---------------|
| Secret management | Environment variables / K8s Secrets â€” never committed to git |
| Non-root containers | Docker images create and run as `appuser` (UID 1000) |
| Rate limiting | Three layers: SlowAPI, Nginx, backpressure middleware |
| Input validation | Pydantic models validate all request bodies |
| Content deduplication | SHA-256 content hash prevents duplicate processing |
| CORS | Configurable middleware (restrict origins in production) |
| Network isolation | Docker Compose internal bridge network |
| Log sanitisation | Sensitive data excluded from structured logs |
| Health endpoints | No sensitive data exposed in `/health` or `/ready` |

---

## ðŸ›£ï¸ Roadmap

- [x] Core RAG pipeline (chunk â†’ embed â†’ index â†’ extract)
- [x] Background processing with Celery (retries, dead-letter, queue routing)
- [x] Multi-tier Redis caching (extraction, embedding, dedup)
- [x] Prometheus + Grafana observability stack
- [x] Structured logging with correlation IDs
- [x] Rate limiting + backpressure middleware
- [x] Abstract vector store interface (FAISS swappable)
- [x] Kubernetes manifests with HPA
- [x] Nginx load balancer + Gunicorn production config
- [x] Full testing pyramid (unit, integration, E2E)
- [x] Comprehensive documentation
- [ ] Multi-LLM provider support (Azure OpenAI, Anthropic, local models)
- [ ] Streaming ingestion & real-time extraction
- [ ] KEDA-based autoscaling (scale workers on queue depth)
- [ ] Helm chart for simplified K8s deployment
- [ ] OpenTelemetry distributed tracing
- [ ] Alembic database migrations

---

## ðŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development setup, coding standards, and PR guidelines.

---

## ðŸ“„ License

This project is for educational and portfolio purposes. See the repository for license details.

---

> Built with â˜• and a passion for production-grade AI systems.

